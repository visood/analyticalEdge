Title
========================================================

##How do we treat categorical variables for a logistic regression?
R will make contrast variables and work with those. But we could have a different representation. The predictions on these variables will just be the conditioned means of the response. We could pretend that the categorical variable is actually a discretized realization of a continuous variable, and for some other data it could assume any other value. In this case we code the categorical variable as discrete numerical values. We could use a prior assumption to make this code, or just use the conditioned means of the response! Lets consider an example
```{r}

n <- 0.25
m <- 0.25
xfunc <- function(size=1) {
  sapply(1:size, function(i){
    u <- runif(n=1)
    if (u < n)  -1
    else {
      if (u < (1-m)) 0
      else 1
    }
  })
}
p <- c("-1"=0.,"0"=0.5, "1"=1.)
yfunc <- function(xs) sapply(xs, function(x)if (runif(n=1) < p[x]) 1 else 0)
numCode <- c("-1"=-1, "0"=0, "1"=1)
N <- 10000
X <- as.factor(xfunc(N))
Y <- yfunc(as.character(X))
XY <- data.frame(X=X, Y=Y)
XY$X <- as.character(XY$X)
freqs <- sapply(unique(XY$X), function(x) sum( XY$X == x))
XY$X <- as.factor(XY$X)
XY$X <-relevel(XY$X, names(which.max(freqs)))
#XY$X <- relevel(XY$X, "0")
XY$num <- numCode[ as.character(XY$X)]
XY.log <- glm( Y ~ X, data=XY, family="binomial")
XY.num.log <- glm(Y ~ num, data=XY, family="binomial")
groupMeans <- sapply(c("-1", "0","1"), function(x) mean(XY$Y[ as.character(XY$X) == x]))
groupSds <- sapply(c("-1", "0","1"), function(x) sd(XY$Y[ as.character(XY$X) == x]))
nd <- data.frame( X=as.factor(c(-1, 0, 1)), num=c(-1, 0, 1), Y=c(0, 1, 0.5))
predFac <- predict(XY.log, newdata=nd, type="response")
predNum <- predict(XY.num.log, newdata=nd, type="response")
cbind(nd, groupMeans, predFac, predNum, groupSds)
summary(XY.log)
summary(XY.num.log)
```
The conclusion here is that the logistic model reduces to the conditional means of the levels of the categarical variable X.
Now let us introduce two categaroical variables as predictors
```{r}
allXY <- as.data.frame(do.call("rbind", lapply(c(-1,0,1), function(x) do.call("rbind", lapply(c(-1,0,1), function(y) c(x,y))) )))
colnames(allXY) <- c("X", "Y")
allXY <- allXY[order( 3*allXY$X + allXY$Y),]
predFun <- function(size=1, p) {
  sapply(1:size, function(i){
    u <- runif(n=1)
    if (u < p[1])  -1
    else {
      if (u < (1-p[2])) 0
      else 1
    }
  })
}
#respFun <- function(xys, p) sapply(xs, function(x)if (runif(n=1) < p[p[,1]==x,2]) 1 else 0)
respFun <- function(xys, p) apply( xys, 1, function(xy) if( runif(n=1) < p[ p$X==xy[1] & p$Y==xy[2], 3]) 1 else 0 )
numCode <- c("-1"=-1, "0"=0, "1"=1)
px <- c(0.25, 0.25)
py <- c(0.25, 0.25)
pz <- data.frame(allXY, Z=apply(allXY, 1,  function(r) (sum(r) + 2)/4))
N <- 10000
X <- predFun(N, px)
Y <- predFun(N, py)
Z <- respFun(data.frame(X=X, Y=Y), pz)
XYZ <- data.frame(X=as.factor(X), Y=as.factor(Y), Z=Z)
XYZ$X <- relevel(XYZ$X, names(which.max(table(XYZ$X))))
XYZ$Y <- relevel(XYZ$Y, names(which.max(table(XYZ$Y))))
XYZ$nX <- numCode[as.character(XYZ$X)]
XYZ$nY <- numCode[as.character(XYZ$Y)]
xyz.log <- glm( Z ~ X + Y, data=XYZ)
xyz.num.log <- glm(Z ~ nX + nY, data=XYZ)
summary(xyz.log)
summary(xyz.num.log)
nd <- allXY
nd$X <- as.factor(nd$X)
nd$Y <- as.factor(nd$Y)
nd$nX <- numCode[as.character(nd$X)]
nd$nY <- numCode[as.character(nd$Y)]

predFac <- predict(xyz.log, newdata=nd, type="response")
predNum <- predict(xyz.num.log, newdata=nd, type="response")

groupMeans <- apply(allXY, 1, function(r) mean( XYZ$Z[ as.character(XYZ$X) == r[1] & as.character(XYZ$Y) == r[2]]))
groupSds <- apply(allXY, 1, function(r) sd( XYZ$Z[ as.character(XYZ$X) == r[1] & as.character(XYZ$Y) == r[2]]))
cbind(ndXY, predFac, predNum, groupMeans, groupSds)
```
Once again we find that the logistic model will just reproduce the observed group means. We could use the logistic model because the group means would fit one. Will this always be the case?
### Will a system with categorical predictors, and binary response always fit to a logistic?
Consider a binary categorical predictor. If the two group means are well separated, the logistic model will work. In fact, this is what the logistic model is trying to do. Think of the logistic model as essentially a linear disriminant model. For continuous variables we assume Gaussian distributions around two means that would correspond to the two response values. 


## Data Massaging
Lets read the data in, and make a train and test set
```{r}
comp <- read.csv("train.csv")
questions <- colnames(comp)[9:109]
library(caTools)
split <- sample.split( comp$Happy, SplitRatio = 0.7)
compTrain <- subset(comp, split==TRUE)
compTest <- subset(comp, split==FALSE)
```

Lets first look at the group  means for each question
```{r}
computeGroupMeanByLevel <- function(q, compdf){
  sapply(levels( compdf[ , q]), function(l) mean( compdf[ compdf[, q] == l, "Happy"]) )
}
        
groupMeans <- data.frame(do.call("rbind", lapply(questions, function(q) computeGroupMeanByLevel(q, compTrain))))
rownames(groupMeans) <- questions
colnames(groupMeans) <- c("blank", "No", "Yes")
hb <- hist(groupMeans$blank, plot=FALSE)
hn <- hist(groupMeans$No, plot=FALSE)
hp <- hist(groupMeans$Yes, plot=FALSE)
plot(hb, freq=FALSE, border="green", xlim=c(0.4, 0.7), main="Histogram of the three responses", xlab="group means")
lines(hn, freq=FALSE, border="blue")
lines(hp, freq=FALSE, border="red")
```
If we do not sort the non-blank reponses, we cannot determine which reponse is correlated with happiness. We could choose the questions that lie on the left and right tails of the above histogram, and work with those questions.

We should determine if the yes or no on a question should be positive or negative towards happiness. This is a subjective prior on the nature of the response, but will not influence the data analysis in anyway. We proceed by making a numerical code for each question, and we will also tag each question.
```{r natureOfTheQuestions}
qidx <- read.csv("questions.csv", stringsAsFactors=FALSE)$Index
#names(questions) <- qidx
qs <- read.csv("questions.csv", stringsAsFactors=FALSE)$Question
qs <- gsub(pattern="/", replacement=" ", qs)
qs <- gsub(pattern="-", replacement=" ", qs)
library(tm)
library(SnowballC)
corpus <- Corpus(VectorSource(qs))
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords("english") ) 
capFirstLetter <- function(x) paste(toupper(substring(x, 1,1)), substring(x, 2), sep="")

qlists <- lapply(corpus, function(x) Filter(function(s) s != "", unlist(strsplit(x, " ", fixed=TRUE))))
qsAbrv <- unlist(lapply(qlists, function(xs) paste(Map(capFirstLetter, xs), collapse = "") ))

```
Now lets abbreviate the answers
```{r questionAnswers}
choices <- read.csv("questions.csv", stringsAsFactors=FALSE)$Choices
choices <- data.frame(do.call("rbind", lapply(strsplit(x=choices, split="/"), function(xs)sapply(xs, function(x)  gsub(pattern=" ", replacement="", x)))))
choices <- t(apply(choices, 1, function(r) c("blank", r)))
colnames(choices) <- c("A0", "A1", "A2")
qs <- as.matrix(cbind( qsAbrv, choices))
rownames(qs) <- qidx
colnames(qs) <- c("Q", "A0", "A1", "A2")

#reorder the rows of qs to confirm to the order in the columns of comp dfs
qs <- qs[questions, ]

toset <- apply(qs, 1, function(r) (r[2] != "Yes") | (r[3] != "No"))
torev <- c(7, 8, 9, 10, 11, 13, 14, 16, 18, 20, 22, 25, 26, 27, 29, 33, 35, 36, 37, 41, 42, 43, 45, 48, 49, 50, 54, 56, 57, 59, 60, 61, 64, 66, 68, 69, 70, 74, 77, 78, 79, 80, 84, 86, 88, 91, 92, 94, 97, 98, 100, 101  )

numCode <- cbind(rep(0, 101), rep(1, 101), rep(-1, 101))
for(i in torev){
  numCode[i,2] <- -1
  numCode[i,3] <- 1
}
rownames(numCode) <- qidx
colnames(numCode) <- c("A0n", "A1n", "A2n")
qsans <- data.frame(qs, numCode)
```
Lets numericize the answers
```{r numericAnswers}
makeNum <- function(q, xs) {
  sapply(xs, function(x){
    if (x=="") 0
    else {
      if( gsub(pattern=" ", replacement="", x) == qsans[q, "A1"]) qsans[q, "A1n"]
      else qsans[q, "A2n"]
    }
  })
}

compTrainNum <- compTrain
compTestNum <- compTest
for( q in questions){
  compTrainNum[,q] <- makeNum(q, compTrainNum[,q])
  compTestNum[,q] <- makeNum(q, compTestNum[,q])
}
```
So we have the answers to the questions as numerics.
Lets do the histograms again.
```{r numhistos}
computeGroupMeanNum <- function(q, compNum){
  sapply( c(-1, 0, 1), function(x) mean( compNum[ compNum[, q] == x, "Happy"]) )
}
computeGroupSdNum <- function(q, compNum){
  sapply( c(-1, 0, 1), function(x) sd( compNum[ compNum[, q] == x, "Happy"]) )
}
groupMeansNum <- data.frame(do.call("rbind", lapply(questions, function(q) computeGroupMeanNum(q, compTrainNum))))
rownames(groupMeansNum) <- questions
colnames(groupMeansNum) <- c("Down", "Flat", "Up")
groupSdsNum <- data.frame(do.call("rbind", lapply(questions, function(q) computeGroupSdNum(q, compTrainNum))))
rownames(groupSdsNum) <- questions
colnames(groupSdsNum) <- c("Down", "Flat", "Up")
hd <- hist(groupMeansNum$Down, plot=FALSE)
hf <- hist(groupMeansNum$Flat, plot=FALSE)
hu <- hist(groupMeansNum$Up, plot=FALSE)

plot(hd, freq=FALSE, border="blue", xlim=c(0.4, 0.7), ylim=c(0,50), main="Histogram of group means", xlab="group means")
lines(hf, freq=FALSE, border="green")
lines(hu, freq=FALSE, border="red")
legend("topleft", legend=c("down", "flat", "up"), col=c("blue", "green", "red"), lty=1, cex=2)

hdl <- hist(log(groupMeansNum$Down), plot=FALSE)
hfl <- hist(log(groupMeansNum$Flat), plot=FALSE)
hul <- hist(log(groupMeansNum$Up), plot=FALSE)
plot(hdl, freq=FALSE, border="blue", xlim=c(-1,-0.3), ylim=c(0,25), main="Histogram of log group means", xlab = "log of group means")
lines(hfl, freq=FALSE, border="green")
lines(hul, freq=FALSE, border="red")
legend("topleft", legend=c("down", "flat", "up"), col=c("blue", "green", "red"), lty=1, cex=2)
```
We can filter the questions by their t-values. Notice one thing, the extreme positive changers have changed since last time. The extreme changers we saw earlier are now sitting in the test set. Care is thus necessary to pick up the relevant questions.


```{r}
N <- nrow(compTrainNum)
groupTs <- sqrt(N)*(groupMeans - colMeans(groupMeans, na.rm=TRUE))/groupSds
groupStats <- cbind(groupMeans, groupSds, groupTs)
hdt <- hist(groupTs$Down, plot=FALSE)
hft <- hist(groupTs$Flat, plot=FALSE)
hut <- hist(groupTs$Up, plot=FALSE)
plot(hdt, border="blue", ylim=c(0, 0.12), xlim=c(-25, 15), freq=FALSE, main="Histogram of t-statistics of group means", xlab="t-statistics of group means")
lines(hft, border="green", freq=FALSE)
lines(hut, border="red", freq=FALSE)
legend("topleft", legend=c("down", "flat", "up"), col=c("blue", "green", "red"), lty=1, cex=2)

sigQsStats.num <- groupStats[ groupTs$Up > 10 | groupTs$Down < -10,]
sigQidx.num <- rownames(sigQsStats)
```
We now have a list of relevant questions after numericizing the answers. Lets use a different method, and then we can compare the results.

### Using group means for each question's levels to determine which choice is negative, which neutral, which positive.
However, we can choose the questions using a different methodology, working directly with the levels, and using the data to determine the downs/flats/ups for each quesiton.
```{r}
qlevels <- t(sapply(questions, function(q) levels(compTrain[,q])))
qlevels[,1] <- "blank"
rownames(qlevels) <- questions
qlevels.list <- lapply(questions, function(q) {ls <- levels(compTrain[,q]);  ls[1] <- "blank"; ls})
qlevels.list <- lapply(qlevels.list, function(ls) { ll <- 1:3; names(ll) <- ls; ll})
names(qlevels.list) <- questions
computeGroupMeanByLevel <- function(q, comp){
  sapply(levels( comp[ , q]), function(l) mean( comp[ comp[, q] == l, "Happy"]) )
}
computeGroupSdByLevel <- function(q, comp){
  sapply(levels( comp[ , q]), function(l) sd( comp[ comp[, q] == l, "Happy"]) )
}
computeGroupSizesByLevel<- function(q, comp){
  sapply(levels( comp[ , q]), function(l) sum( comp[,q] == l)) #length( comp[ comp[, q] == l, "Happy"]) )
}
        
groupSizes <- data.frame(do.call("rbind", lapply(questions, function(q) computeGroupSizesByLevel(q, compTrain))))
groupMeans <- data.frame(do.call("rbind", lapply(questions, function(q) computeGroupMeanByLevel(q, compTrain))))
groupSds <- data.frame(do.call("rbind", lapply(questions, function(q) computeGroupSdByLevel(q, compTrain))))
rownames(groupMeans) <- questions
rownames(groupSizes) <- questions
```

make some plots
```{r}
makeGroupMeanSDPlot <- function(i, gm, gs, fromLevels=FALSE){
  m <- as.numeric(gm[i,])
  s <- as.numeric(gs[i,])
  ylim <- c(min(m - s) - 0.10, max(m + s) + 0.1)
  xlim <- c(min(m) - 0.05, max(m) + 0.05)
  plot(m, m , col=c("red", "green", "blue"), ylim=ylim, xlim=xlim, cex=2, pch=19, main= qs[i, 1])
  points(m, m - s, col=c("red", "green", "blue"))
  points(m, m + s, col=c("red", "green", "blue"))
  abline(h=(m-s), col=c("red", "green", "blue"), lty=1)
  abline(h=m, col=c("red", "green", "blue"), lty=1, lwd=2)
  abline(h=(m+s), col=c("red", "green", "blue", lty=1))
  if (fromLevels){
    text(x = m,  y = m - 0.01, labels=qlevels[i,], col=c("red", "green", "blue"))
  }
  else text(x=m, y=m-0.01, labels=c("Down", "Flat", "Up"), col=c("red", "green", "blue") )
  print("energies")
  #print(paste(questionEnergyLevels(i, gm)))
  questionEnergyLevels(i, gm)
}
discrimination <- function(i, gm, gs){
  m <- as.numeric(gm[i,])
  s <- as.numeric(gs[i,])
  ord <- order(m)
  m <- m[ord]
  s <- s[ord]
  d <- c( m[2] - s[2] - m[1] - s[1], m[3] - s[3] - m[2] -s[2])
  names(d) <- as.character(qlevels[i, ord][c(1,3)])
  d
}
happyMean <- mean(compTrain$Happy)
questionEnergyLevels <- function(q, gm){
  m <- as.numeric(gm[q,])
  -log(m/(1-m)) + log(happyMean/(1-happyMean))
}
  
  
groupMeanSds <- sqrt(2*groupMeans/groupSizes)
sigQs <- questions[ds[,1] > 0 & ds[,2] > 0]

computeGroupMeanByLevel <- function(q, comp){
  sapply(levels( comp[ , q]), function(l) mean( comp[ comp[, q] == l, "Happy"]) )
}

energyFromMean <- function(m) -log(m/(1-m)) + log(happyMean/(1-happyMean))
computeGroupEnergyByLevel <- function(q, comp){
  sapply(levels( comp[, q]), function(l) meanEnergy( mean(comp[comp[,q]])))

groupEnergies <- t(apply(groupMeans, 1, energyFromMean))
qlevel.list <- lapply( qlevels, function(ls) 1:3)
totalEnergy0 <- function(anss){ # anss should bea  named list with questions as names
  n <- length(anss)
  qns <- names(anss)
  qls <- qlevels.list[qns]
  js <- as.numeric( sapply(1:n, function(i) qls[[i]][anss[i]]))
  #sapply(1:n, function(i) groupEnergies[qns[i], js[i]])
  mean(diag(groupEnergies[qns, js]))
}

#i did not realize that invoking as.numeric on factor data will return the factor levels! this makes our life simpler
totalEnergy <- function(anss){ #anss is a vector of factor variables
  qns <- names(anss)
  levs <- as.numeric(anss)
  sum(diag(groupEnergies[qns, levs])) - log(happyMean/(1-happyMean))
}
meanEnergy <- function(anss){ #anss is a vector of factor variables
  totalEnergy(anss)/length(anss)
}
  
```

We can order the choices for a question by their group means! Thus obtaining a numerical code from the data, and then see how well we did when we ordered the responses subjectively.
```{r}
numCodeByLevelsList<- lapply(questions, function(q){
  m <- as.numeric(groupMeans[q,])
  ord <- order(m)
  code0 <- c(-1, 0, 1)
  code <- code0[ord]
  names(code) <- c("blank", levels(compTrain[,q])[c(2,3)])
  code
})
numCodeByLevels <- as.matrix(do.call("rbind", numCodeByLevelsList))
qsansByLevel <- data.frame(qs, numCodeByLevels)
colnames(qsansByLevel) <- c("Q", "L0", "L1", "L2", "L0n", "L1n", "L2n")

numOrd <- t(apply(groupMeans,1,  order))
groupMeansLevNum <- data.frame(t(sapply(1:101, function(i) as.numeric(groupMeans[i, numOrd[i,]]))))
groupSizesLevNum <-  data.frame(t(sapply(1:101, function(i) as.numeric(groupSizes[i, numOrd[i,]]))))
colnames(groupMeansLevNum) <- c("Down", "Flat", "Up")
colnames(groupSizesLevNum) <- c("Down", "Flat", "Up")
rownames(groupMeansLevNum) <- rownames(groupMeans)
rownames(groupSizesLevNum) <- rownames(groupSizes)
groupMeanSdsLevNum <- sqrt(2*groupMeansLevNum/groupSizesLevNum)
groupMeansLevNum.withOrdering <- data.frame(groupMeansLevNum, qsansByLevel[, -1])

```
We now have data-defined numerical order on the levels, and group means accordingly sorted.
We can compute and plot the histograms again.
```{r}
hd <- hist(as.numeric(groupMeansLevNum$Down), plot=FALSE)
hf <- hist(as.numeric(groupMeansLevNum$Flat), plot=FALSE)
hu <- hist(as.numeric(groupMeansLevNum$Up), plot=FALSE)
plot(hd, freq=FALSE, border="blue", xlim=c(0.4, 0.7), ylim=c(0,60), main="Histogram of group means", xlab="group means")
lines(hf, freq=FALSE, border="green")
lines(hu, freq=FALSE, border="red")
legend("topleft", legend=c("down", "flat", "up"), col=c("blue", "green", "red"), lty=1, cex=2)
```

## Energy analysis
Before we plunge into predicting, lets see what the energy levels for each question tell us about the data we have
```{r}
q.ord <- data.frame(t(apply(groupEnergies, 1, order)))
ge.ord <- data.frame(t(apply(groupEnergies, 1, sort)))
ge.ord <- ge.ord[order(ge.ord[,3]),]
plot(ge.ord[,1], ylim = c(-0.6, 0.6), col="blue", main="energy levels for questions ordered by largest energy", ylab="energy")
points(ge.ord[,2], ylim = c(-0.6, 0.6), col="green")
points(ge.ord[,3], ylim = c(-0.6, 0.6), col="red")

ge.ord <- ge.ord[order(ge.ord[,1], decreasing = TRUE),]
plot(ge.ord[,1], ylim = c(-0.6, 0.6), col="blue", main="energy levels for questions ordered by lowest energy", ylab="energy")
points(ge.ord[,2], ylim = c(-0.6, 0.6), col="green")
points(ge.ord[,3], ylim = c(-0.6, 0.6), col="red")

ge.ord <- ge.ord[ order(ge.ord[,3] - ge.ord[,1]), ]
plot(ge.ord[,1], ylim = c(-0.6, 0.6), col="blue", main="energy levels for questions ordered by max - min", ylab="energy", cex=3)
points(ge.ord[,2], ylim = c(-0.6, 0.6), col="green", cex=3)
points(ge.ord[,3], ylim = c(-0.6, 0.6), col="red", cex=3)
text(x=1:nrow(ge.ord), y=ge.ord[,1], labels=(1:nrow(ge.ord)), col="blue", cex = 0.8)
text(x=1:nrow(ge.ord), y=ge.ord[,2], labels=(1:nrow(ge.ord)), col="green", cex = 0.8)
text(x=1:nrow(ge.ord), y=ge.ord[,3], labels=(1:nrow(ge.ord)), col="red", cex = 0.8)

#groupSizes

gs.ord <-  t(sapply(1:nrow(groupSizes), function(i) groupSizes[i, as.numeric(q.ord[i,])]))
gs.ord <- data.frame(unlist(gs.ord[,1]), unlist(gs.ord[,2]), unlist(gs.ord[,3]))
gs.ord <- gs.ord[order(ge.ord[,3] - ge.ord[,1]),]
plot(gs.ord[,1], ylim = c(100, 2200), col="blue", main="energy levels for questions ordered by max - min", ylab="energy", cex=3)
points(gs.ord[,2],  col="green", cex=3)
points(gs.ord[,3],  col="red", cex=3)
text(x=1:nrow(gs.ord), y=gs.ord[,1], labels=(1:nrow(gs.ord)), col="blue", cex = 0.8)
text(x=1:nrow(gs.ord), y=gs.ord[,2], labels=(1:nrow(gs.ord)), col="green", cex = 0.8)
text(x=1:nrow(gs.ord), y=gs.ord[,3], labels=(1:nrow(gs.ord)), col="red", cex = 0.8)

gsds.ord <-  t(sapply(1:nrow(groupMeanSds), function(i) groupMeanSds[i, as.numeric(q.ord[i,])]))
gsds.ord <- data.frame(unlist(gsds.ord[,1]), unlist(gsds.ord[,2]), unlist(gsds.ord[,3]))
gsds.ord <- gsds.ord[order(ge.ord[,3] - ge.ord[,1]),]
plot(gsds.ord[,1], ylim = c(0.02, 0.09), col="blue", main="energy levels for questions ordered by max - min", ylab="energy", cex=3)
points(gsds.ord[,2],  col="green", cex=3)
points(gsds.ord[,3],  col="red", cex=3)
text(x=1:nrow(gsds.ord), y=gsds.ord[,1], labels=(1:nrow(gsds.ord)), col="blue", cex = 0.8)
text(x=1:nrow(gsds.ord), y=gsds.ord[,2], labels=(1:nrow(gsds.ord)), col="green", cex = 0.8)
text(x=1:nrow(gsds.ord), y=gsds.ord[,3], labels=(1:nrow(gsds.ord)), col="red", cex = 0.8)
```
###Predictions Using Energies
```{r}
train.happy <- compTrain$Happy
test.happy <- compTest$Happy
qds <- Filter( function(q){ dq <- discrimination(q, groupMeans, groupMeanSds); dq[1] > 0 & dq[2] > 0}, questions)
qdsDscrm <- t(sapply(qds, function(q)discrimination(q, groupMeans, groupMeanSds)))
qds <- qds[order(qdsDscrm[,2])]
qsDscrm <- t(sapply(questions, function(q)discrimination(q, groupMeans, groupMeanSds)))
questions.dscrmOrdered <- questions[order(qsDscrm[,2])]

probFromEnergy <- function(e) 1/(1+exp(e))
makeTableForQs <- function(qs, comp){
    es <- sapply(1:nrow(comp), function(i) totalEnergy(comp[i, qs]))
    tb <- table(comp$Happy, probFromEnergy(es) > 0.5)
    print( paste("accuracy", sum(diag(tb))/sum(tb)))
    tb
}
accuracyForQs <- function(qs, comp){
  es <- sapply(1:nrow(comp), function(i) totalEnergy(comp[i, qs]))
  tb <- table(comp$Happy, probFromEnergy(es) > 0.5)
  sum(diag(tb))/sum(tb)
}
```
We can also use the order on questions defined by the max/min level energies for each questions.
```{r}
questions.deltaEnergyOrdered <- questions[order(rowMax(groupEnergies) - rowMin(groupEnergies))]
nqs <- seq(80,101, 1)
aqs.train <- sapply(nqs, function(n) accuracyForQs(qdeo[n:101], compTrain))
aqs.test <- sapply(nqs, function(n) accuracyForQs(qdeo[n:101], compTest))
plot(101-nqs, 1-aqs.train, col="blue", type="b", main="prediction error", xlab="number of questions", ylab="1-accuracy")
points(101-nqs, 1-aqs.test, col="red", type="b")
legend("topright", legend=c("train", "test"), col=c("blue", "red"), pch=1, cex=2)
```

Using uncoupled energies we cannot do better than an accuracy of 0.67 on the training set. May be we should get more pedestrian and try the simple canned logistic model
### Logistic model
Simply use the can on all variables
```{r}
comp.log <- glm(Happy ~ ., data=compTrain, family="binomial" )
#significant questions,
lqs <- c( "Q120194", "Q120014", "Q119334",  "Q118237", "Q115899", "Q107869", "Q102674", "Q102687", "Q102289","Q102089", "Q99716" )
aqs.train.lqs <- sapply(nqs, function(n) accuracyForQs(lqs, compTrain))
pred.log <- predict(comp.log, newdata=compTest, type="response")      
tb <- table(compTest$Happy, pred.log > 0.5)
sum(diag(tb))/sum(tb)
```
Raw logistic model is less accurate than our energy based method. We can combine the two, by feeding the energy disriminated questions to the logistic model, or the group mean discriminated questions,
```{r}
compTrain$HouseholdStatus <- relevel(compTrain$HouseholdStatus, "Single (no kids)")
accLogisitic <- function(qs){
  comp.log <- glm(Happy ~ ., data= compTrain[, c(qs,  "Happy")], family="binomial")
  pred.train.log <- predict(comp.log, type="response")
  pred.test.log <- predict(comp.log, newdata=compTest, type="response")
  tbtrain <- table(compTrain$Happy, pred.train.log > 0.5)
  tbtest <- table(compTest$Happy, pred.test.log > 0.5)
  c(train=sum(diag(tbtrain))/sum(tbtrain), test=sum(diag(tbtest))/sum(tbtest))
  
}
```

###Clustering
Interactions between variables will show up in clustering. HouseholdStatus is important, but clustering cannot handle this factor variable. So lets numericize it in the same way as we did the questions. 
```{r}
hhls <- levels(compTrain$HouseholdStatus)
hhlMeans <- sapply(hhls, function(l) mean(compTrain$Happy[ compTrain$HouseholdStatus==l])) 
hhlSizes <- sapply(hhls, function(l) sum( compTrain$HouseholdStatus==l)) 
hhEns <- energyFromMean(hhlMeans)
hhln <- order(hhEns)
hhln[hhln] <- seq(-3, 3,)
names(hhln) <- hhls
compTrainNum$HouseholdStatus <- sapply(compTrain$HouseholdStatus, function(l) as.numeric(hhln[l]))
```
Now we can cluster
```{r}
cluCompTrain <- hclust(dist(compTrainNum[, c(qdeo[95:101], "HouseholdStatus")], method="euclidean"), method="ward")
clusters <- cutree(cluCompTrain, k = 8)
t(sapply(1:8, function(n) c( mean(compTrain$Happy[clusters==n]), sum(clusters==n))))


